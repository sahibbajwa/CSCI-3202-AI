{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Fall 2020\n",
    "\n",
    "# Friday November 20, 2020\n",
    "\n",
    "# In-class notebook:  (Passive?) Q-Learning\n",
    "\n",
    "<a id='top'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Shortcuts:  [Top](#top) || [Setup](#setup) || [Training](#train) || [OOP](#oop) || [Wrapup](#end) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first half of the code below is adapted from https://blog.floydhub.com/an-introduction-to-q-learning-reinforcement-learning/, an example by Sayak Paul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "<a/ id='setup'></a>\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a *route-finding* problem.  Suppose we can move across the maze given where at any state we can move to the adjacent states, if not blocked by walls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the states\n",
    "location_to_state = {\n",
    "    'L1' : 0,\n",
    "    'L2' : 1,\n",
    "    'L3' : 2,\n",
    "    'L4' : 3,\n",
    "    'L5' : 4,\n",
    "    'L6' : 5,\n",
    "    'L7' : 6,\n",
    "    'L8' : 7,\n",
    "    'L9' : 8\n",
    "}\n",
    "\n",
    "# Define the actions\n",
    "actions = [0,1,2,3,4,5,6,7,8]\n",
    "\n",
    "# Define the rewards\n",
    "rewards = np.array([[0,1,0,0,0,0,0,0,0],\n",
    "              [1,0,1,0,1,0,0,0,0],\n",
    "              [0,1,0,0,0,1,0,0,0],\n",
    "              [0,0,0,0,0,0,1,0,0],\n",
    "              [0,1,0,0,0,0,0,1,0],\n",
    "              [0,0,1,0,0,0,0,0,0],\n",
    "              [0,0,0,1,0,0,0,1,0],\n",
    "              [0,0,0,0,1,0,1,0,1],\n",
    "              [0,0,0,0,0,0,0,1,0]])\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'L1',\n",
       " 1: 'L2',\n",
       " 2: 'L3',\n",
       " 3: 'L4',\n",
       " 4: 'L5',\n",
       " 5: 'L6',\n",
       " 6: 'L7',\n",
       " 7: 'L8',\n",
       " 8: 'L9'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maps indices to locations\n",
    "state_to_location = dict((state,location) for location,state in location_to_state.items())\n",
    "state_to_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "gamma = 0.75 # Discount factor \n",
    "alpha = 0.9 # Learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Training:\n",
    "\n",
    "<a/ id='train'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a temporal-difference agent that updates Q-values along the way.  This agent will:\n",
    "\n",
    "- have a desired goal state, with large positive rewards associated with reaching it\n",
    "- have training episodes:pick a state at random and undertake exactly one move from it.  Update Q-values.\n",
    "- Moves will be non-random: there is no transition probability matrix, and $P(s'| a \\, s)=1$ for the desired successor state\n",
    "- Eventually try to pick out the best move available at any given tile: or the shortest path from any start location to the goal state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.         2249.5           0.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [1688.125         0.         2998.            0.         1688.125\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.         2249.5        3996.            0.            0.\n",
      "  2249.5           0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.          951.3203125     0.            0.        ]\n",
      " [   0.         2249.5           0.            0.            0.\n",
      "     0.            0.         1267.09375       0.        ]\n",
      " [   0.            0.         2998.            0.            0.\n",
      "     0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.          714.49023437    0.\n",
      "     0.            0.         1267.09375       0.        ]\n",
      " [   0.            0.            0.            0.         1688.125\n",
      "     0.          951.3203125     0.          951.3203125 ]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.            0.         1267.09375       0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ending_state=(2)\n",
    "\n",
    "# Initializing Q-Values\n",
    "Q = np.array(np.zeros([9,9]))\n",
    "# reward the exit/end location\n",
    "rewards_new=np.copy(rewards)\n",
    "rewards_new[ending_state,ending_state] = 999\n",
    "\n",
    "for i in range(10000):\n",
    "    # Pick up a state randomly\n",
    "    current_state = np.random.randint(0,9)\n",
    "\n",
    "    playable_actions = []\n",
    "# Iterate through the new rewards matrix and get the actions > 0\n",
    "#realistically this should live in its own dictionary/matrix first thing, and NOT BE RECOMPUTED\n",
    "    for j in range(9):\n",
    "        if rewards_new[current_state,j] > 0:\n",
    "            playable_actions.append(j)\n",
    "#     print(current_state, playable_actions)\n",
    "    next_state = np.random.choice(playable_actions) #random choice of action for the learning\n",
    "    \n",
    "    # Compute the Q difference between current state and where we ended up\n",
    "    QD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n",
    "\n",
    "    # Update the Q-Value for the action taken using the Bellman equation\n",
    "    Q[current_state,next_state] += alpha * QD\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_route(start_location,end_location, Q):\n",
    "    route = [start_location]\n",
    "    # We do not know about the next location yet, so initialize with the value of \n",
    "    # starting location\n",
    "    next_location = start_location\n",
    "    \n",
    "    # We don't know about the exact number of iterations\n",
    "    # needed to reach to the final location hence while loop will be a good choice \n",
    "    # for iteratiing\n",
    "    \n",
    "    while(next_location != end_location):\n",
    "        # Fetch the starting state\n",
    "        starting_state = location_to_state[start_location]\n",
    "        \n",
    "        # Fetch the highest Q-value pertaining to starting state\n",
    "        next_state = np.argmax(Q[starting_state,])\n",
    "        \n",
    "        # We got the index of the next state. But we need the corresponding letter. \n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        \n",
    "        # Update the starting location for the next iteration\n",
    "        start_location = next_location\n",
    "    \n",
    "    return route\n",
    "\n",
    "get_optimal_route('L5','L3',Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that function will currently only guaranteed work if the end_location is 'L3', since that's the `end_location=2` \n",
    "we rewarded and used to solve for the Q-values.  Here's the above all glued into one function, so we can choose any start and end location: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_route(start_location,end_location):\n",
    "    # Copy the rewards matrix to new Matrix\n",
    "    rewards_new = np.copy(rewards)\n",
    "    \n",
    "    # Get the ending state corresponding to the ending location as given\n",
    "    ending_state = location_to_state[end_location]\n",
    "    \n",
    "    # With the above information automatically set the priority of  \n",
    "    # the given ending state to the highest one\n",
    "    rewards_new[ending_state,ending_state] = 999\n",
    "\n",
    "    # -----------Q-Learning algorithm-----------\n",
    "   \n",
    "    # Initializing Q-Values\n",
    "    Q = np.array(np.zeros([9,9]))\n",
    "\n",
    "    # Q-Learning process\n",
    "    for i in range(1000):\n",
    "        # Pick up a state randomly\n",
    "        current_state = np.random.randint(0,9) # Python excludes the upper bound\n",
    "        \n",
    "        # For traversing through the neighbor locations in the maze\n",
    "        playable_actions = []\n",
    "        \n",
    "        # Iterate through the new rewards matrix and get the actions > 0\n",
    "        for j in range(9):\n",
    "            if rewards_new[current_state,j] > 0:\n",
    "                playable_actions.append(j)\n",
    "        \n",
    "        # Pick an action randomly from the list of playable actions  \n",
    "        # leading us to the next state\n",
    "        next_state = np.random.choice(playable_actions)\n",
    "        \n",
    "        # Compute the Q-difference between where we started and where we ended up.\n",
    "        QD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]\n",
    "        \n",
    "        # Update the Q-Value using the Bellman equation\n",
    "        Q[current_state,next_state] += alpha * QD\n",
    "\n",
    "     # Initialize the optimal route with the starting location\n",
    "    route = [start_location]\n",
    "    # We do not know about the next location yet, so initialize with the value of \n",
    "    # starting location\n",
    "    next_location = start_location\n",
    "    \n",
    "    # We don't know about the exact number of iterations\n",
    "    # needed to reach to the final location hence while loop will be a good choice \n",
    "    # for iteratiing\n",
    "    \n",
    "    while(next_location != end_location):\n",
    "        # Fetch the starting state\n",
    "        starting_state = location_to_state[start_location]\n",
    "        \n",
    "        # Fetch the highest Q-value pertaining to starting state\n",
    "        next_state = np.argmax(Q[starting_state,])\n",
    "        \n",
    "        # We got the index of the next state. But we need the corresponding letter. \n",
    "        next_location = state_to_location[next_state]\n",
    "        route.append(next_location)\n",
    "        \n",
    "        # Update the starting location for the next iteration\n",
    "        start_location = next_location\n",
    "    \n",
    "    return route\n",
    "\n",
    "get_optimal_route('L9','L2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## An OOP rendition:\n",
    "\n",
    "<a/ id='oop'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could also create a class for the agent, that needs all of these things in intialization and spans a broader class of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \n",
    "    # Initialize alpha, gamma, states, actions, rewards, and Q-values\n",
    "    def __init__(self, alpha, gamma, location_to_state, actions, rewards, state_to_location, Q):\n",
    "        \n",
    "        self.gamma = gamma  \n",
    "        self.alpha = alpha \n",
    "        \n",
    "        self.location_to_state = location_to_state\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.state_to_location = state_to_location\n",
    "        \n",
    "        self.Q = Q\n",
    "        \n",
    "    # Training\n",
    "    def training(self, start_location, end_location, iterations):\n",
    "        \n",
    "        rewards_new = np.copy(self.rewards)\n",
    "        \n",
    "        ending_state = self.location_to_state[end_location]\n",
    "        rewards_new[ending_state, ending_state] = 999\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            current_state = np.random.randint(0,9) \n",
    "            playable_actions = []\n",
    "\n",
    "            for j in range(9):\n",
    "                if rewards_new[current_state,j] > 0:\n",
    "                    playable_actions.append(j)\n",
    "    \n",
    "            next_state = np.random.choice(playable_actions)\n",
    "            QD = rewards_new[current_state,next_state] + \\\n",
    "                    self.gamma * self.Q[next_state, np.argmax(self.Q[next_state,])] - self.Q[current_state,next_state]\n",
    "            \n",
    "            self.Q[current_state,next_state] += self.alpha * QD\n",
    "\n",
    "        route = [start_location]\n",
    "        next_location = start_location\n",
    "        \n",
    "        # Get the route \n",
    "        self.get_optimal_route(start_location, end_location, next_location, route, self.Q)\n",
    "        \n",
    "    # Get the optimal route\n",
    "    def get_optimal_route(self, start_location, end_location, next_location, route, Q):\n",
    "        \n",
    "        while(next_location != end_location):\n",
    "            starting_state = self.location_to_state[start_location]\n",
    "            next_state = np.argmax(Q[starting_state,])\n",
    "            next_location = self.state_to_location[next_state]\n",
    "            route.append(next_location)\n",
    "            start_location = next_location\n",
    "        \n",
    "        print(route)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qagent = QAgent(alpha, gamma, location_to_state, actions, rewards,  state_to_location, Q)\n",
    "qagent.training('L9', 'L1', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Roundup:\n",
    "\n",
    "<a/ id='end'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few reasons this code might be unsatisfactory, and only could handle a very small subset of problems we want a `QAgent()` to handle!  We might have to adjust it for a variety of things in a real-world problem that isn't just route-finding on an open grid.\n",
    "\n",
    "**Not perfectly observable state space**, or **Real Training**\n",
    "\n",
    "In the above code, our \"training episodes\" consisted of exactly one move each.  This is atypical!  It's more common to have some kind of double loop, where the inner loop is an entire route-find including random actions and the outer loop repeats that experiment.  This would lead to a variable amount of Q-updates per trip through the inner loop!\n",
    "\n",
    "We need those extra episodes for two reasons:\n",
    "\n",
    "- many agents don't get to start with all locations/states initialized.  They have to explore the state and add to those dicitonaries as new states are discovered!  We would want a function that appends as we go.\n",
    "- Allowing the agent to explore route sequentially allows to eventually prioritize **exploitation** over just **exploration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Truly Stochastic movement**\n",
    "\n",
    "In the example above, we used `np.random.choice(playable_actions)` to choose an action, and then immediately retrieved `next_state` from it.  More realistically, we might want some stochastic component, where there's a function that tells us next state as a function of action, but it's probabilitistic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_state(state, movement):\n",
    "    #Function that returns the result of *ACTUAL* movement; e.g.\n",
    "    #if movement = up:\n",
    "        #return (state[0], state[1]+1)\n",
    "    #if movement = right:\n",
    "        #return (state[0]+1, state[1])\n",
    "    return\n",
    "\n",
    "#here's one we've seen before: Prob=p we take the desired action, 1-p we randomly get another action\n",
    "def transition(state, action, playable_actions,p):\n",
    "    roll=np.random.random()\n",
    "    \n",
    "    if roll<p:\n",
    "        print(state, action)\n",
    "        return new_state(state,action)\n",
    "    if roll>p:\n",
    "        result_action=np.random.choice(playable_actions) #or maybe playable actions exluding \"action\"\n",
    "        print(state, result_action)\n",
    "        return new_state(state,result_action)\n",
    "\n",
    "for i in range(20):\n",
    "    transition(0,1,[2,3], .80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we do this, we still update the Q-values corresponding to `(state, action)`, but the \"other\" location which we use to increment rewards and calculate the Q-difference is the output state of the `transition` function, not the Q value of where we \"tried\" to move.\n",
    "\n",
    "Crucially the `new_state` function is the one that describes the **actual process** if the problem is one that we don't always know how things result.  This is often hidden from our agent: what will the computer do/the screen look like if we push \"right\" on the controller in a game of Pac-Man?  This is in the next_state portion of the model, and could require us actually just implement the action given by transition and record what actually happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Exploration rather than truly random**\n",
    "\n",
    "Not only is the movement itself deterministic in our opening problem, our agent is using `np.random.choice(playable_actions)` to choose which state-action pair to update!\n",
    "\n",
    "In reality, we often want to encourage our agent to persue rewards, especially as the sample space may become too large to brute-force all possible state-action pairs.\n",
    "\n",
    "A more appropriate implementation of that choice might be to create a function and a new dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actstaken={}\n",
    "\n",
    "for location,state in location_to_state.items():\n",
    "    for location,action in location_to_state.items():\n",
    "        actstaken[(state,action)]=0\n",
    "        \n",
    "actstaken\n",
    "\n",
    "#inflate utility by a multiple until we've taken that action \"ne\" times\n",
    "def explorationfunc(utility, actstaken,ne=10):\n",
    "    if actstaken>ne:\n",
    "        return utility\n",
    "    else: return utility*10\n",
    "print(actstaken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we would replace the `np.random.choice(playable_actions)` with a function that evaluates the 'explorationfunc' at each of the playable actions, and chooses that one.  After we observe the `next_state` from that action, we would update the actstaken counter accordingly.  And possible also the utility in the same dictionary!\n",
    "\n",
    "\n",
    "For this problem tuples of the form `(state1, state2)` work as actions.  This led to a quare matrix or an $n^2$ size dictionary above.  More generally, a similar problem would work with tuples of the form `(location, direction)`, where each location has only a few *actions* associated with it, like \"Up\", \"Down\", \"Left\", \"Right\", etc.\n",
    "\n",
    "In reality, we could hold a *single* dictionary thats of the form:\n",
    "\n",
    "actions={\n",
    "\n",
    "(0,'right'): [Qval, ne],\n",
    "\n",
    "(1,'left'): [Qval, ne],\n",
    "\n",
    "(1,'down'): [Qval, ne],\n",
    "\n",
    "(1,'right'): [Qval, ne],\n",
    "\n",
    "...}\n",
    "\n",
    "Or we could nest the dictionaries, so the structure becomes:\n",
    "\n",
    "actions={\n",
    "\n",
    "0: {'right': [Qval, ne]},\n",
    "\n",
    "1: {'left': [Qval, ne], 'down': [Qval, ne], 'right': [Qval, ne]},\n",
    "\n",
    "...}\n",
    "\n",
    "\n",
    "The nested dictionaries is particularly appealing if the `state` tuples start becoming more complex, like multivariate data.  We'll play with an example where `state=(position, velocity)` in a future example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
