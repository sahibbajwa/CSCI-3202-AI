{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Fall 2020\n",
    "\n",
    "# Wednesday November 25, 2020\n",
    "\n",
    "# In-class notebook:  Gym and Active Learning\n",
    "\n",
    "<a id='top'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Shortcuts:  [Top](#top) || [Frozen Lake](#lake) || [Cart-Pole](#pole) ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "#### Last time we did a notebook in class for describing a path-finding agent.  It was doing a variant of Q-learning that was nearly the same as the temporal-difference model, because there was no stochastic (random) component of movement.  \n",
    "\n",
    "There were a few takeaways from that model.\n",
    "\n",
    "    - It didn't have to *learn* exploration, since it had a full yobservable state space or maze.\n",
    "    - It chose which action to take at random, rather than prioritizing exploitation after sufficient iterations.\n",
    "    - It created a (sparse) matrix of (state, state) pairs as a proxy for a true set of (state,action) pairs.  This is a bad habit for two reasons: poor memory allocation and that it doesn't generalize to many problems.  Often our actions are things like \"press the accelerator,\" which is a little different than \"try to move to known state #3459.\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `gym` Environment\n",
    "\n",
    "The Python package `gym` is a nice one that designed with Q-learning in mind, and contains a large set of example problems.  You may find its documentation at https://gym.openai.com/, and it can be installed via pip (pip3) or directly in conda/anaconda.  It is loaded below with numpy, matplot lib.\n",
    "\n",
    "Rendering in Jupyter is sketchy, so I'll be running any cells that animate directly as Python .py notebooks in IDLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('c:/users/zacha/appdata/local/packages/pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0/localcache/local-packages/python37/site-packages')\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Frozen Lake\n",
    "\n",
    "This is a path-finding problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Discrete(4)\n",
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.render()\n",
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are navigating an 4x4 grid (in `self.observation_space`).  The states are denoted as:\n",
    "- S: Start\n",
    "- G: Goal\n",
    "- F: Frozen Ice\n",
    "- H: A hole in the ice.  We fall through and get very cold.\n",
    "\n",
    "At any state, we can choose one of 4 actions from (in `self.action_space`):\n",
    "- LEFT = 0\n",
    "- DOWN = 1\n",
    "- RIGHT = 2\n",
    "- UP = 3\n",
    "\n",
    "For each spot on the grid above, we have a probability associated with each action.\n",
    "\n",
    ".P returns:\n",
    "\n",
    "- a probability of a successor\n",
    "- the actual successor\n",
    "- the reward of that successor\n",
    "- whether or not we're \"done\" with the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "print(env.P[0][0])\n",
    "# print(env.P[1][2])\n",
    "# print(env.P[14][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What `gym`does nicely for us is it both saves those state spaces above and allows us to take a `step()` of the random process.  This is the same as out `transition` on the homework, and describes how the random process evolves.  It also includes all of the crucial measurements we want.  Per the official documentation, `step` returns four values. These are:\n",
    "\n",
    "\n",
    "\n",
    "1. `observation` (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "2. `reward` (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "3. `done` (boolean): whether it’s time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "4.`info` (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "## Making Q:\n",
    "In general, Q needs to be a thing that can be evaluated for each state-action pair.  \n",
    "\n",
    "- Since each state is at least calling from only a small set of possible actions, one option is to create a *rectangular* array where each row is a state and each column is the Q-values associated for each action.  We'd still have to be careful to never attempt to choose an action that's invalid however, as this will include possibilities like moving \"Up\" when we're already in the top row.\n",
    "- Another option is to create a full dictionary or nested dictionaries, where the first key is the state, the second key is the action, and the resulting list or array holds the Q-values, number of times taken, and any other needed information about that action.\n",
    "\n",
    "I will do the first here, but if the number of possible actions is *huge* and you can only take a few of those actions in any *given* state, this would be a very memory inefficient allocation and we'd want dictionaries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 500\n",
      "Episode: 1000\n",
      "Episode: 1500\n",
      "Episode: 2000\n",
      "Episode: 2500\n",
      "Episode: 3000\n",
      "Episode: 3500\n",
      "Episode: 4000\n",
      "Episode: 4500\n",
      "Episode: 5000\n",
      "Episode: 5500\n",
      "Episode: 6000\n",
      "Episode: 6500\n",
      "Episode: 7000\n",
      "Episode: 7500\n",
      "Episode: 8000\n",
      "Episode: 8500\n",
      "Episode: 9000\n",
      "Episode: 9500\n",
      "Episode: 10000\n",
      "Episode: 10500\n",
      "Episode: 11000\n",
      "Episode: 11500\n",
      "Episode: 12000\n",
      "Episode: 12500\n",
      "Episode: 13000\n",
      "Episode: 13500\n",
      "Episode: 14000\n",
      "Episode: 14500\n",
      "Episode: 15000\n",
      "Episode: 15500\n",
      "Episode: 16000\n",
      "Episode: 16500\n",
      "Episode: 17000\n",
      "Episode: 17500\n",
      "Episode: 18000\n",
      "Episode: 18500\n",
      "Episode: 19000\n",
      "Episode: 19500\n",
      "Episode: 20000\n",
      "Episode: 20500\n",
      "Episode: 21000\n",
      "Episode: 21500\n",
      "Episode: 22000\n",
      "Episode: 22500\n",
      "Episode: 23000\n",
      "Episode: 23500\n",
      "Episode: 24000\n",
      "Episode: 24500\n",
      "Episode: 25000\n",
      "Episode: 25500\n",
      "Episode: 26000\n",
      "Episode: 26500\n",
      "Episode: 27000\n",
      "Episode: 27500\n",
      "Episode: 28000\n",
      "Episode: 28500\n",
      "Episode: 29000\n",
      "Episode: 29500\n",
      "Episode: 30000\n",
      "Episode: 30500\n",
      "Episode: 31000\n",
      "Episode: 31500\n",
      "Episode: 32000\n",
      "Episode: 32500\n",
      "Episode: 33000\n",
      "Episode: 33500\n",
      "Episode: 34000\n",
      "Episode: 34500\n",
      "Episode: 35000\n",
      "Episode: 35500\n",
      "Episode: 36000\n",
      "Episode: 36500\n",
      "Episode: 37000\n",
      "Episode: 37500\n",
      "Episode: 38000\n",
      "Episode: 38500\n",
      "Episode: 39000\n",
      "Episode: 39500\n",
      "Episode: 40000\n",
      "Episode: 40500\n",
      "Episode: 41000\n",
      "Episode: 41500\n",
      "Episode: 42000\n",
      "Episode: 42500\n",
      "Episode: 43000\n",
      "Episode: 43500\n",
      "Episode: 44000\n",
      "Episode: 44500\n",
      "Episode: 45000\n",
      "Episode: 45500\n",
      "Episode: 46000\n",
      "Episode: 46500\n",
      "Episode: 47000\n",
      "Episode: 47500\n",
      "Episode: 48000\n",
      "Episode: 48500\n",
      "Episode: 49000\n",
      "Episode: 49500\n",
      "Episode: 50000\n",
      "Episode: 50500\n",
      "Episode: 51000\n",
      "Episode: 51500\n",
      "Episode: 52000\n",
      "Episode: 52500\n",
      "Episode: 53000\n",
      "Episode: 53500\n",
      "Episode: 54000\n",
      "Episode: 54500\n",
      "Episode: 55000\n",
      "Episode: 55500\n",
      "Episode: 56000\n",
      "Episode: 56500\n",
      "Episode: 57000\n",
      "Episode: 57500\n",
      "Episode: 58000\n",
      "Episode: 58500\n",
      "Episode: 59000\n",
      "Episode: 59500\n",
      "Episode: 60000\n",
      "Episode: 60500\n",
      "Episode: 61000\n",
      "Episode: 61500\n",
      "Episode: 62000\n",
      "Episode: 62500\n",
      "Episode: 63000\n",
      "Episode: 63500\n",
      "Episode: 64000\n",
      "Episode: 64500\n",
      "Episode: 65000\n",
      "Episode: 65500\n",
      "Episode: 66000\n",
      "Episode: 66500\n",
      "Episode: 67000\n",
      "Episode: 67500\n",
      "Episode: 68000\n",
      "Episode: 68500\n",
      "Episode: 69000\n",
      "Episode: 69500\n",
      "Episode: 70000\n",
      "Episode: 70500\n",
      "Episode: 71000\n",
      "Episode: 71500\n",
      "Episode: 72000\n",
      "Episode: 72500\n",
      "Episode: 73000\n",
      "Episode: 73500\n",
      "Episode: 74000\n",
      "Episode: 74500\n",
      "Episode: 75000\n",
      "Episode: 75500\n",
      "Episode: 76000\n",
      "Episode: 76500\n",
      "Episode: 77000\n",
      "Episode: 77500\n",
      "Episode: 78000\n",
      "Episode: 78500\n",
      "Episode: 79000\n",
      "Episode: 79500\n",
      "Episode: 80000\n",
      "Episode: 80500\n",
      "Episode: 81000\n",
      "Episode: 81500\n",
      "Episode: 82000\n",
      "Episode: 82500\n",
      "Episode: 83000\n",
      "Episode: 83500\n",
      "Episode: 84000\n",
      "Episode: 84500\n",
      "Episode: 85000\n",
      "Episode: 85500\n",
      "Episode: 86000\n",
      "Episode: 86500\n",
      "Episode: 87000\n",
      "Episode: 87500\n",
      "Episode: 88000\n",
      "Episode: 88500\n",
      "Episode: 89000\n",
      "Episode: 89500\n",
      "Episode: 90000\n",
      "Episode: 90500\n",
      "Episode: 91000\n",
      "Episode: 91500\n",
      "Episode: 92000\n",
      "Episode: 92500\n",
      "Episode: 93000\n",
      "Episode: 93500\n",
      "Episode: 94000\n",
      "Episode: 94500\n",
      "Episode: 95000\n",
      "Episode: 95500\n",
      "Episode: 96000\n",
      "Episode: 96500\n",
      "Episode: 97000\n",
      "Episode: 97500\n",
      "Episode: 98000\n",
      "Episode: 98500\n",
      "Episode: 99000\n",
      "Episode: 99500\n",
      "Episode: 100000\n",
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "alpha = 0.2\n",
    "gamma = 0.2\n",
    "epsilon = 0.2\n",
    "env = gym.make('FrozenLake-v0')\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "# states=np.array([])\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # EXPLORE action space at prob epsilon\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # else EXPLOIT learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max) #Q update formula, discounted\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "    if i % 500 == 0:\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.26006117e-07 6.47653984e-07 1.89301501e-06 5.56768076e-07]\n",
      "[['R' 'D' 'R' 'L']\n",
      " ['D' 'L' 'R' 'L']\n",
      " ['R' 'D' 'D' 'L']\n",
      " ['L' 'D' 'D' 'L']]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "print(q_table[0])\n",
    "def policy_to_dir(policy):\n",
    "    if policy==0: return 'L'\n",
    "    elif policy==1: return 'D'\n",
    "    elif policy==2: return 'R'\n",
    "    elif policy==3: return 'U'\n",
    "dirs=np.array([policy_to_dir(np.argmax(q_table[st])) for st in range(16)])\n",
    "\n",
    "dirs.shape=(4,4)\n",
    "print(dirs)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Now that we have a Q, we can watch an agent go!\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state]) # Exploit learned values\n",
    "    next_state, reward, done, info = env.step(action) \n",
    "    state = next_state\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Cartpole: Dealing with Continuity\n",
    "\n",
    "This is a balancing problem! We ran a quick demo, which can be run outside Jupyter if you wish to visualize it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CELL WILL NOT WORK IN JUPYTER\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In order, the 4 characteristics of the state space are:\n",
    " \n",
    "- Cart Position             -4.8                    4.8\n",
    "- Cart Velocity             -Inf                    Inf\n",
    "- Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "- Pole Angular Velocity     -Inf                    Inf\n",
    "\n",
    "\n",
    "And we can choose 2 discrete moves.\n",
    "- 0    to Push cart to the left\n",
    "- 1    to  Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print(env.observation_space)\n",
    "\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a continuous state space!  Q-learning requires discrete action-state pairs, so maybe we make a function to bin observations into regions instead.  A binning function just chops the state space into regions like $7<x<8$ and assigns all $x$ values within that region to a single discrete value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4791666674945089  proportion of the way through the space\n",
      "4791 bin index\n"
     ]
    }
   ],
   "source": [
    "#consider binning on the first category...\n",
    "bins=10000\n",
    "obs=-.2\n",
    "minv=env.observation_space.low[0]\n",
    "maxv=env.observation_space.high[0]\n",
    "prop=(obs-minv)/(maxv-minv) #numerator: distance from min value\n",
    "                            #denominator: out of total length of continuous space\n",
    "print(prop, ' proportion of the way through the space')\n",
    "obs_bin=int(round((bins-1)*prop))\n",
    "print(obs_bin, 'bin index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5 4.8 -4.8\n",
      "1 5 100.0 -100.0\n",
      "-0.3 10 0.41887903 -0.41887903\n",
      "0 10 100.0 -100.0\n",
      "(2, 2, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "#now maybe all 4 at once.  Let's cap velocities  at 100.\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "mins=env.observation_space.low\n",
    "mins[1]=-100\n",
    "mins[3]=-100\n",
    "maxs=env.observation_space.high\n",
    "maxs[1]=100\n",
    "maxs[3]=100\n",
    "\n",
    "def discretize_state(bins, state_min, state_max, obs):\n",
    "    discretized=list()\n",
    "    for i in range(len(obs)):\n",
    "        prop=((obs[i]-state_min[i])/\n",
    "                 (state_max[i]-state_min[i]))\n",
    "        print(obs[i], bins[i], state_max[i], state_min[i])\n",
    "        obs_bin=int(round((bins[i]-1)*prop))\n",
    "        discretized.append(obs_bin)\n",
    "    return tuple(discretized)\n",
    "\n",
    "print(discretize_state((5,5,10,10), mins,maxs, [0,1,-.3,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the same format as before, we may also want the ability to \"flatten\" our 4D state space into a single 1D list of actions that we can pair up with the two actions.  There are a handful of ways to do this, but `np.reshape` is one way to take a higher dimensional array and push the indices to another location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "[[0 1 2]\n",
      " [3 4 5]]\n",
      "[[0]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]\n",
      " [5]]\n",
      "[[[ 0  1  2]\n",
      "  [ 3  4  5]\n",
      "  [ 6  7  8]\n",
      "  [ 9 10 11]]\n",
      "\n",
      " [[12 13 14]\n",
      "  [15 16 17]\n",
      "  [18 19 20]\n",
      "  [21 22 23]]]\n",
      "reshapes to \n",
      "[[ 0]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]\n",
      " [13]\n",
      " [14]\n",
      " [15]\n",
      " [16]\n",
      " [17]\n",
      " [18]\n",
      " [19]\n",
      " [20]\n",
      " [21]\n",
      " [22]\n",
      " [23]]\n"
     ]
    }
   ],
   "source": [
    "#reshaping multivariate data into a single list\n",
    "a = np.arange(6).reshape((3, 2))\n",
    "print(a)\n",
    "\n",
    "\n",
    "print(np.reshape(a, (2, 3)))\n",
    "\n",
    "#for making a larger array into a single list, we'd make one of the elements of the reshape \"1\"\n",
    "print(np.reshape(a, (6, 1)))\n",
    "\n",
    "#...which now has each index in it's own row of a vector.\n",
    "\n",
    "\n",
    "#In higher dimensions:\n",
    "b = np.arange(24).reshape((2, 4,3))\n",
    "print(b)\n",
    "print('reshapes to ')\n",
    "print(np.reshape(b, (24,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'll be wanting to move from one index to the other often, you may also create a couple of dictonaries of the form `1D-to-ND` and vice-versa where you can input the integer key of the `1D` array to get the coordinate tuple OR a input the tuple and get the integer value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
